{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c92af-63d5-4809-a77a-3ec81ee97063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import col, mean, column\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992f215-2349-4ba2-bdbf-f122a391ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PCAExample1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e46c6-bc4a-4e44-9b68-b5250956b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.csv(\"wildfire100.csv\" , header = True, inferSchema = True)\n",
    "column_names = df_raw.columns\n",
    "\n",
    "df_raw = df_raw.drop(\"acq_date\")\n",
    "df_raw = df_raw.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9608d4c-1a91-46ca-a97f-aac4f7454291",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = df_raw.columns\n",
    "assembler = VectorAssembler(inputCols = feature_columns, outputCol =  \"features\")\n",
    "assembled_data = assembler.transform(df_raw)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(assembled_data)\n",
    "\n",
    "scaled_data = scaler_model.transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2859a0-61f8-4b41-a404-59c63285c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of original features\n",
    "num_features = len(feature_columns) # feature_columns from your original dataframe\n",
    "\n",
    "#num_features + 1\n",
    "\n",
    "explained_variances = []\n",
    "\n",
    "for num_components in range(1, ):\n",
    "    pca = PCA(k = num_components , inputCol=\"scaled_features\", outputCol=\"pcaFeatures\")\n",
    "    pca_model = pca.fit(scaled_data)\n",
    "    var = pca_model.explainedVariance\n",
    "    explained_variances.append(var.sum())\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_features + 1), explained_variances, marker='o')\n",
    "plt.title('Explained Variance by Number of Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds410_f23)",
   "language": "python",
   "name": "ds410_f23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
