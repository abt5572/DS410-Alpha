{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import col, mean, column\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "#from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from decision_tree_plot.decision_tree_parser import decision_tree_parse\n",
    "from decision_tree_plot.decision_tree_plot import plot_trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code from here below is what was used within cluster mode to find the best hyperparameters for the decision tree, this code was initially meant to be used for the whole 22GB dataset. \n",
    "#Right now the outputs only show us testing the code on the small dataset, but in cluster mode we did use the full 5GB version of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName(\"ProjectTree1\").getOrCreate()\n",
    "#ss = SparkSession.builder.config(\"spark.driver.memory\", \"5g\").master(\"local\").appName(\"PCAExample1\").getOrCreate()\n",
    "#ss = SparkSession.builder.master(\"local\").appName(\"PCAExample1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setCheckpointDir(\"/storage/home/sxs6549/work/Project/scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.9 ms, sys: 12.3 ms, total: 56.2 ms\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "df_raw = ss.read.csv(\"wildfiredb.csv\", header=True, inferSchema=True)\n",
    "#df_raw = spark.read.csv(\"wildfire100.csv\" , header = True, inferSchema = True)\n",
    "#column_names = df_raw.columns\n",
    "\n",
    "#df_raw = df_raw.drop(\"acq_date\")\n",
    "df_raw = df_raw.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.6 ms, sys: 3.78 ms, total: 29.4 ms\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#df_raw_trial = ss.read.csv(\"fire_small.csv\", header=True, inferSchema=True)\n",
    "#df_raw = spark.read.csv(\"wildfire100.csv\" , header = True, inferSchema = True)\n",
    "#column_names = df_raw_trial.columns\n",
    "\n",
    "#df_raw = df_raw.drop(\"acq_date\")\n",
    "#df_raw_trial = df_raw_trial.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_list = list(df_raw.columns)\n",
    "#col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_list_new = list(set(col_list) - set(['_c0', 'Polygon_ID', 'acq_date', 'frp']))\n",
    "#col_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_columns = df_raw.columns\n",
    "#col_list = list(df_raw.columns)\n",
    "col_list = list(df_raw.columns)\n",
    "feature_inputs = list(set(col_list) - set(['_c0', 'Polygon_ID', 'acq_date', 'frp']))\n",
    "\n",
    "assembler_tree = VectorAssembler(inputCols = feature_inputs, outputCol =  \"features\")\n",
    "assembled_data_tree = assembler_tree.transform(df_raw)\n",
    "#scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "#scaler_model = scaler.fit(assembled_data)\n",
    "\n",
    "#scaled_data = scaler_model.transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_tree = PCA(k=36, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model_tree = pca_tree.fit(assembled_data_tree)\n",
    "result_tree = model_tree.transform(assembled_data_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, Polygon_ID: int, acq_date: date, frp: double, Neighbour: int, Neighbour_frp: double, CBC2014_max: double, CBC2014_min: double, CBC2014_median: double, CBC2014_sum: double, CBC2014_mode: double, CBC2014_count: int, CBC2014_mean: double, CBD2012_max: double, CBD2012_min: double, CBD2012_median: double, CBD2012_sum: double, CBD2012_mode: double, CBD2012_count: int, CBD2012_mean: double, CBD2016_max: double, CBD2016_min: double, CBD2016_median: double, CBD2016_sum: double, CBD2016_mode: double, CBD2016_count: int, CBD2016_mean: double, CBH2012_max: double, CBH2012_min: double, CBH2012_median: double, CBH2012_sum: double, CBH2012_mode: double, CBH2012_count: int, CBH2012_mean: double, CBH2014_max: double, CBH2014_min: double, CBH2014_median: double, CBH2014_sum: double, CBH2014_mode: double, CBH2014_count: int, CBH2014_mean: double, CBH2016_max: double, CBH2016_min: double, CBH2016_median: double, CBH2016_sum: double, CBH2016_mode: double, CBH2016_count: int, CBH2016_mean: double, CC2012_max: double, CC2012_min: double, CC2012_median: double, CC2012_sum: double, CC2012_mode: double, CC2012_count: int, CC2012_mean: double, CC2014_max: double, CC2014_min: double, CC2014_median: double, CC2014_sum: double, CC2014_mode: double, CC2014_count: int, CC2014_mean: double, CC2016_max: double, CC2016_min: double, CC2016_median: double, CC2016_sum: double, CC2016_mode: double, CC2016_count: int, CC2016_mean: double, CH2012_max: double, CH2012_min: double, CH2012_median: double, CH2012_sum: double, CH2012_mode: double, CH2012_count: int, CH2012_mean: double, CH2014_max: double, CH2014_min: double, CH2014_median: double, CH2014_sum: double, CH2014_mode: double, CH2014_count: int, CH2014_mean: double, CH2016_max: double, CH2016_min: double, CH2016_median: double, CH2016_sum: double, CH2016_mode: double, CH2016_count: int, CH2016_mean: double, ELEV2016_max: double, ELEV2016_min: double, ELEV2016_median: double, ELEV2016_sum: double, ELEV2016_mode: double, ELEV2016_count: int, ELEV2016_mean: double, EVC2012_max: double, EVC2012_min: double, EVC2012_median: double, EVC2012_sum: double, EVC2012_mode: double, EVC2012_count: int, EVC2012_mean: double, EVC2014_max: double, EVC2014_min: double, EVC2014_median: double, EVC2014_sum: double, EVC2014_mode: double, EVC2014_count: int, EVC2014_mean: double, EVC2016_max: double, EVC2016_min: double, EVC2016_median: double, EVC2016_sum: double, EVC2016_mode: double, EVC2016_count: int, EVC2016_mean: double, EVH2012_max: double, EVH2012_min: double, EVH2012_median: double, EVH2012_sum: double, EVH2012_mode: double, EVH2012_count: int, EVH2012_mean: double, EVH2014_max: double, EVH2014_min: double, EVH2014_median: double, EVH2014_sum: double, EVH2014_mode: double, EVH2014_count: int, EVH2014_mean: double, EVH2016_max: double, EVH2016_min: double, EVH2016_median: double, EVH2016_sum: double, EVH2016_mode: double, EVH2016_count: int, EVH2016_mean: double, EVT2012_max: double, EVT2012_min: double, EVT2012_median: double, EVT2012_sum: double, EVT2012_mode: double, EVT2012_count: int, EVT2012_mean: double, EVT2014_max: double, EVT2014_min: double, EVT2014_median: double, EVT2014_sum: double, EVT2014_mode: double, EVT2014_count: int, EVT2014_mean: double, EVT2016_max: double, EVT2016_min: double, EVT2016_median: double, EVT2016_sum: double, EVT2016_mode: double, EVT2016_count: int, EVT2016_mean: double, FCCS2014_max: double, FCCS2014_min: double, FCCS2014_median: double, FCCS2014_sum: double, FCCS2014_mode: double, FCCS2014_count: int, FCCS2014_mean: double, FVC2016_max: double, FVC2016_min: double, FVC2016_median: double, FVC2016_sum: double, FVC2016_mode: double, FVC2016_count: int, FVC2016_mean: double, FVH2016_max: double, FVH2016_min: double, FVH2016_median: double, FVH2016_sum: double, FVH2016_mode: double, FVH2016_count: int, FVH2016_mean: double, FVT2016_max: double, FVT2016_min: double, FVT2016_median: double, FVT2016_sum: double, FVT2016_mode: double, FVT2016_count: int, FVT2016_mean: double, NVC2016_max: double, NVC2016_min: double, NVC2016_median: double, NVC2016_sum: double, NVC2016_mode: double, NVC2016_count: int, NVC2016_mean: double, SLP2016_max: double, SLP2016_min: double, SLP2016_median: double, SLP2016_sum: double, SLP2016_mode: double, SLP2016_count: int, SLP2016_mean: double, Neighbour_CBC2014_max: double, Neighbour_CBC2014_min: double, Neighbour_CBC2014_median: double, Neighbour_CBC2014_sum: double, Neighbour_CBC2014_mode: double, Neighbour_CBC2014_count: int, Neighbour_CBC2014_mean: double, Neighbour_CBD2012_max: double, Neighbour_CBD2012_min: double, Neighbour_CBD2012_median: double, Neighbour_CBD2012_sum: double, Neighbour_CBD2012_mode: double, Neighbour_CBD2012_count: int, Neighbour_CBD2012_mean: double, Neighbour_CBD2016_max: double, Neighbour_CBD2016_min: double, Neighbour_CBD2016_median: double, Neighbour_CBD2016_sum: double, Neighbour_CBD2016_mode: double, Neighbour_CBD2016_count: int, Neighbour_CBD2016_mean: double, Neighbour_CBH2012_max: double, Neighbour_CBH2012_min: double, Neighbour_CBH2012_median: double, Neighbour_CBH2012_sum: double, Neighbour_CBH2012_mode: double, Neighbour_CBH2012_count: int, Neighbour_CBH2012_mean: double, Neighbour_CBH2014_max: double, Neighbour_CBH2014_min: double, Neighbour_CBH2014_median: double, Neighbour_CBH2014_sum: double, Neighbour_CBH2014_mode: double, Neighbour_CBH2014_count: int, Neighbour_CBH2014_mean: double, Neighbour_CBH2016_max: double, Neighbour_CBH2016_min: double, Neighbour_CBH2016_median: double, Neighbour_CBH2016_sum: double, Neighbour_CBH2016_mode: double, Neighbour_CBH2016_count: int, Neighbour_CBH2016_mean: double, Neighbour_CC2012_max: double, Neighbour_CC2012_min: double, Neighbour_CC2012_median: double, Neighbour_CC2012_sum: double, Neighbour_CC2012_mode: double, Neighbour_CC2012_count: int, Neighbour_CC2012_mean: double, Neighbour_CC2014_max: double, Neighbour_CC2014_min: double, Neighbour_CC2014_median: double, Neighbour_CC2014_sum: double, Neighbour_CC2014_mode: double, Neighbour_CC2014_count: int, Neighbour_CC2014_mean: double, Neighbour_CC2016_max: double, Neighbour_CC2016_min: double, Neighbour_CC2016_median: double, Neighbour_CC2016_sum: double, Neighbour_CC2016_mode: double, Neighbour_CC2016_count: int, Neighbour_CC2016_mean: double, Neighbour_CH2012_max: double, Neighbour_CH2012_min: double, Neighbour_CH2012_median: double, Neighbour_CH2012_sum: double, Neighbour_CH2012_mode: double, Neighbour_CH2012_count: int, Neighbour_CH2012_mean: double, Neighbour_CH2014_max: double, Neighbour_CH2014_min: double, Neighbour_CH2014_median: double, Neighbour_CH2014_sum: double, Neighbour_CH2014_mode: double, Neighbour_CH2014_count: int, Neighbour_CH2014_mean: double, Neighbour_CH2016_max: double, Neighbour_CH2016_min: double, Neighbour_CH2016_median: double, Neighbour_CH2016_sum: double, Neighbour_CH2016_mode: double, Neighbour_CH2016_count: int, Neighbour_CH2016_mean: double, Neighbour_ELEV2016_max: double, Neighbour_ELEV2016_min: double, Neighbour_ELEV2016_median: double, Neighbour_ELEV2016_sum: double, Neighbour_ELEV2016_mode: double, Neighbour_ELEV2016_count: int, Neighbour_ELEV2016_mean: double, Neighbour_EVC2012_max: double, Neighbour_EVC2012_min: double, Neighbour_EVC2012_median: double, Neighbour_EVC2012_sum: double, Neighbour_EVC2012_mode: double, Neighbour_EVC2012_count: int, Neighbour_EVC2012_mean: double, Neighbour_EVC2014_max: double, Neighbour_EVC2014_min: double, Neighbour_EVC2014_median: double, Neighbour_EVC2014_sum: double, Neighbour_EVC2014_mode: double, Neighbour_EVC2014_count: int, Neighbour_EVC2014_mean: double, Neighbour_EVC2016_max: double, Neighbour_EVC2016_min: double, Neighbour_EVC2016_median: double, Neighbour_EVC2016_sum: double, Neighbour_EVC2016_mode: double, Neighbour_EVC2016_count: int, Neighbour_EVC2016_mean: double, Neighbour_EVH2012_max: double, Neighbour_EVH2012_min: double, Neighbour_EVH2012_median: double, Neighbour_EVH2012_sum: double, Neighbour_EVH2012_mode: double, Neighbour_EVH2012_count: int, Neighbour_EVH2012_mean: double, Neighbour_EVH2014_max: double, Neighbour_EVH2014_min: double, Neighbour_EVH2014_median: double, Neighbour_EVH2014_sum: double, Neighbour_EVH2014_mode: double, Neighbour_EVH2014_count: int, Neighbour_EVH2014_mean: double, Neighbour_EVH2016_max: double, Neighbour_EVH2016_min: double, Neighbour_EVH2016_median: double, Neighbour_EVH2016_sum: double, Neighbour_EVH2016_mode: double, Neighbour_EVH2016_count: int, Neighbour_EVH2016_mean: double, Neighbour_EVT2012_max: double, Neighbour_EVT2012_min: double, Neighbour_EVT2012_median: double, Neighbour_EVT2012_sum: double, Neighbour_EVT2012_mode: double, Neighbour_EVT2012_count: int, Neighbour_EVT2012_mean: double, Neighbour_EVT2014_max: double, Neighbour_EVT2014_min: double, Neighbour_EVT2014_median: double, Neighbour_EVT2014_sum: double, Neighbour_EVT2014_mode: double, Neighbour_EVT2014_count: int, Neighbour_EVT2014_mean: double, Neighbour_EVT2016_max: double, Neighbour_EVT2016_min: double, Neighbour_EVT2016_median: double, Neighbour_EVT2016_sum: double, Neighbour_EVT2016_mode: double, Neighbour_EVT2016_count: int, Neighbour_EVT2016_mean: double, Neighbour_FCCS2014_max: double, Neighbour_FCCS2014_min: double, Neighbour_FCCS2014_median: double, Neighbour_FCCS2014_sum: double, Neighbour_FCCS2014_mode: double, Neighbour_FCCS2014_count: int, Neighbour_FCCS2014_mean: double, Neighbour_FVC2016_max: double, Neighbour_FVC2016_min: double, Neighbour_FVC2016_median: double, Neighbour_FVC2016_sum: double, Neighbour_FVC2016_mode: double, Neighbour_FVC2016_count: int, Neighbour_FVC2016_mean: double, Neighbour_FVH2016_max: double, Neighbour_FVH2016_min: double, Neighbour_FVH2016_median: double, Neighbour_FVH2016_sum: double, Neighbour_FVH2016_mode: double, Neighbour_FVH2016_count: int, Neighbour_FVH2016_mean: double, Neighbour_FVT2016_max: double, Neighbour_FVT2016_min: double, Neighbour_FVT2016_median: double, Neighbour_FVT2016_sum: double, Neighbour_FVT2016_mode: double, Neighbour_FVT2016_count: int, Neighbour_FVT2016_mean: double, Neighbour_NVC2016_max: double, Neighbour_NVC2016_min: double, Neighbour_NVC2016_median: double, Neighbour_NVC2016_sum: double, Neighbour_NVC2016_mode: double, Neighbour_NVC2016_count: int, Neighbour_NVC2016_mean: double, Neighbour_SLP2016_max: double, Neighbour_SLP2016_min: double, Neighbour_SLP2016_median: double, Neighbour_SLP2016_sum: double, Neighbour_SLP2016_mode: double, Neighbour_SLP2016_count: int, Neighbour_SLP2016_mean: double, features: vector, pcaFeatures: vector]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from decision_tree_plot.decision_tree_parser import decision_tree_parse\n",
    "from decision_tree_plot.decision_tree_plot import plot_trees\n",
    "# Split the data into training and test sets (20% held out for testing)\n",
    "(trainingData, testingData) = result_tree.randomSplit([0.8, 0.2], seed=1237)\n",
    "\n",
    "#Code cell for Part 7\n",
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['max_depth', 'minInstancesPerNode', 'training_rmse', 'testing_rmse',  'Best Model'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "lowest_testing_rmse = 100000\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "max_depth_list = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "minInstancesPerNode_list = [2, 3, 4, 5, 6, 7]\n",
    "#max_depth_list = [2]\n",
    "#minInstancesPerNode_list = [9]\n",
    "#labelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)\n",
    "#feature_inputs = list(set(col_list) - set(['_c0', 'Polygon_ID', 'acq_date', 'frp']))\n",
    "#assembler = VectorAssembler( inputCols=feature_inputs, outputCol=\"features\")\n",
    "#labelConverter = IndexToString(inputCol = \"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "model_path=\"/storage/home/sxs6549/work/Project/fire_DTmodel_vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is  8 , best minInstancesPerNode =  4 , testing rmse =  51.34257362303159\n",
      "CPU times: user 1.41 s, sys: 313 ms, total: 1.72 s\n",
      "Wall time: 4min 12s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "for max_depth in max_depth_list:\n",
    "    for minInsPN in minInstancesPerNode_list:\n",
    "        trainingData.persist()\n",
    "        testingData.persist()\n",
    "        \n",
    "        seed = 37\n",
    "        # Construct a DT model using a set of hyper-parameter values and training data\n",
    "        #dt= DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth= max_depth, minInstancesPerNode= minInsPN)\n",
    "        dt = DecisionTreeRegressor(labelCol=\"frp\", featuresCol=\"pcaFeatures\", maxDepth=max_depth, minInstancesPerNode=minInsPN)\n",
    "        #pipeline = Pipeline(stages=[labelIndexer, assembler, dt, predictionConverter])\n",
    "        model = dt.fit(trainingData)\n",
    "        training_predictions = model.transform(trainingData)\n",
    "        testing_predictions = model.transform(testingData)\n",
    "        #evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        evaluator = RegressionEvaluator(labelCol=\"frp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        training_rmse = evaluator.evaluate(training_predictions)\n",
    "        testing_rmse = evaluator.evaluate(testing_predictions)\n",
    "        # We use 0 as default value of the 'Best Model' column in the Pandas DataFrame.\n",
    "        # The best model will have a value 1000\n",
    "        hyperparams_eval_df.loc[index] = [ max_depth, minInsPN, training_rmse, testing_rmse, 0]  \n",
    "        index = index +1\n",
    "        if testing_rmse < lowest_testing_rmse :\n",
    "            best_max_depth = max_depth\n",
    "            best_minInsPN = minInsPN\n",
    "            best_index = index -1\n",
    "            best_parameters_training_rmse = training_rmse\n",
    "            best_DTmodel= model\n",
    "            best_tree = decision_tree_parse(best_DTmodel, ss, model_path)\n",
    "            column = dict( [ (str(idx), i) for idx, i in enumerate(feature_inputs) ])           \n",
    "            lowest_testing_rmse = testing_rmse\n",
    "print('The best max_depth is ', best_max_depth, ', best minInstancesPerNode = ', \\\n",
    "      best_minInsPN, ', testing rmse = ', lowest_testing_rmse) \n",
    "column = dict([(str(idx), i) for idx, i in enumerate(feature_inputs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_predictions.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code cell for Part 7\n",
    "best_model_path_part7=\"/storage/home/sxs6549/work/Project/fire_DT_HPT_cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code cell for Part 7\n",
    "best_tree=decision_tree_parse(best_DTmodel, ss, best_model_path_part7)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(feature_inputs)])\n",
    "plot_trees(best_tree, column = column, output_path = '/storage/home/sxs6549/work/Project/fire_DT_HPT_cluster.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code cell for Part 7\n",
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_max_depth, best_minInsPN, best_parameters_training_rmse, lowest_testing_rmse, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/sxs6549/work/Project/fire_HPT_cluster.csv\"\n",
    "hyperparams_eval_df.to_csv(output_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python\n(ds410_f23)",
   "language": "python",
   "name": "ds410_f23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
